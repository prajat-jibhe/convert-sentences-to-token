# convert-sentences-to-token
It will convert text and sentences to token and labelling them as 0 non abusive and 1 as abusive
Its for:
1)English
2)German
3)Spanish

It will also create synthetic data,for 1000 counts that will help if your dataset lacks.
